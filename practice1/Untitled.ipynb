{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc22dacb-7297-4f5b-af77-c5f700ad7807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 16:24:30.409041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import torch\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vision_transformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_vit import ViT\n",
    "from timm import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6857501-cc06-4809-8a00-9d9d9bcf3b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../../share/objrecognition/norm\"\n",
    "\n",
    "files = os.listdir(path)\n",
    "dataset = []\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(path, file)\n",
    "    if os.path.isdir(file_path):\n",
    "        images = os.listdir(file_path)\n",
    "        for image in images:\n",
    "            img = Image.open(os.path.join(file_path, image))\n",
    "            img = img.copy()\n",
    "            img = img.convert(\"RGB\")\n",
    "            dataset.append([img, int(file.split()[0][1])])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfbcb023-b94c-4178-9de8-4d54f30960aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)\n",
    "train_dataset = dataset[0:1001]\n",
    "val_dataset = dataset[1001:1141]\n",
    "test_dataset = dataset[1141:1281]\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30cc946f-e7fe-4d45-b37a-c6360c18ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_x = []\n",
    "train_dataset_y = []\n",
    "val_dataset_x = []\n",
    "val_dataset_y = []\n",
    "test_dataset_x = []\n",
    "test_dataset_y = []\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    train_dataset_x.append(train_dataset[i][0])\n",
    "    train_dataset_y.append(train_dataset[i][1])\n",
    "for i in range(len(val_dataset)):\n",
    "    val_dataset_x.append(val_dataset[i][0])\n",
    "    val_dataset_y.append(val_dataset[i][1])\n",
    "for i in range(len(test_dataset)):\n",
    "    test_dataset_x.append(test_dataset[i][0])\n",
    "    test_dataset_y.append(test_dataset[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5923e59-63b4-4ba2-a707-bec4ce4894db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=270x330>, 0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_x[0], train_dataset_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "219e8f9b-c3fe-4e4c-b545-f09a39b4bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, img_transform):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.img_transform = img_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        sample = self.img_transform(sample)\n",
    "        label = self.labels[idx]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a260078-a06f-4b98-aaa2-d5f481809dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(train_dataset_x, train_dataset_y, transform)\n",
    "val_dataset = CustomDataset(val_dataset_x, val_dataset_y, transform)\n",
    "test_dataset = CustomDataset(test_dataset_x, test_dataset_y, transform)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d3b86-cfd4-4073-9a73-d1f2a9f168de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vision_transformer\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "total_accuracy = 0.0\n",
    "total_samples = 0\n",
    "for inputs, labels in val_dataloader:\n",
    "    outputs = model(inputs)\n",
    "    predictions = F.softmax(outputs, dim=-1)\n",
    "    predicted_classes = torch.argmax(predictions, dim=-1)\n",
    "    accuracy = (predicted_classes == labels).float().mean()\n",
    "    total_accuracy += accuracy.item() * inputs.size(0)\n",
    "    total_samples += inputs.size(0)\n",
    "\n",
    "print(\"Validation Accuracy:\", total_accuracy / total_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
